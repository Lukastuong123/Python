{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing Pyspark \n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Spark\n",
    "from pyspark import SparkConf, SparkContext                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the context\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the def function and run the whole thing through the def to spit out \n",
    "input = sc.textFile(\"c:/spark/README.md\")\n",
    "words = input.flatMap(lambda x: x.split())\n",
    "wordCounts = words.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1\n",
      "Apache 1\n",
      "Spark 15\n",
      "is 7\n",
      "a 9\n",
      "unified 1\n",
      "analytics 1\n",
      "engine 2\n",
      "for 12\n",
      "large-scale 1\n",
      "data 2\n",
      "processing. 2\n",
      "It 2\n",
      "provides 1\n",
      "high-level 1\n",
      "APIs 1\n",
      "in 6\n",
      "Scala, 1\n",
      "Java, 1\n",
      "Python, 2\n",
      "and 9\n",
      "R, 1\n",
      "an 4\n",
      "optimized 1\n",
      "that 2\n",
      "supports 2\n",
      "general 2\n",
      "computation 1\n",
      "graphs 1\n",
      "analysis. 1\n",
      "also 5\n",
      "rich 1\n",
      "set 2\n",
      "of 5\n",
      "higher-level 1\n",
      "tools 1\n",
      "including 4\n",
      "SQL 2\n",
      "DataFrames, 1\n",
      "MLlib 1\n",
      "machine 1\n",
      "learning, 1\n",
      "GraphX 1\n",
      "graph 1\n",
      "processing, 1\n",
      "Structured 1\n",
      "Streaming 1\n",
      "stream 1\n",
      "<https://spark.apache.org/> 1\n",
      "[![Jenkins 1\n",
      "Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7) 1\n",
      "[![AppVeyor 1\n",
      "Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark) 1\n",
      "[![PySpark 1\n",
      "Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site) 1\n",
      "## 9\n",
      "Online 1\n",
      "Documentation 1\n",
      "You 4\n",
      "can 7\n",
      "find 1\n",
      "the 24\n",
      "latest 1\n",
      "documentation, 1\n",
      "programming 1\n",
      "guide, 1\n",
      "on 7\n",
      "[project 1\n",
      "web 1\n",
      "page](https://spark.apache.org/documentation.html). 1\n",
      "This 2\n",
      "README 1\n",
      "file 1\n",
      "only 1\n",
      "contains 1\n",
      "basic 1\n",
      "setup 1\n",
      "instructions. 1\n",
      "Building 1\n",
      "built 1\n",
      "using 5\n",
      "[Apache 1\n",
      "Maven](https://maven.apache.org/). 1\n",
      "To 2\n",
      "build 4\n",
      "its 1\n",
      "example 3\n",
      "programs, 1\n",
      "run: 1\n",
      "./build/mvn 1\n",
      "-DskipTests 1\n",
      "clean 1\n",
      "package 1\n",
      "(You 1\n",
      "do 2\n",
      "not 1\n",
      "need 1\n",
      "to 16\n",
      "this 1\n",
      "if 4\n",
      "you 4\n",
      "downloaded 1\n",
      "pre-built 1\n",
      "package.) 1\n",
      "more 1\n",
      "than 1\n",
      "one 3\n",
      "thread 1\n",
      "by 1\n",
      "-T 1\n",
      "option 1\n",
      "with 4\n",
      "Maven, 1\n",
      "see 4\n",
      "[\"Parallel 1\n",
      "builds 1\n",
      "Maven 1\n",
      "3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3). 1\n",
      "More 1\n",
      "detailed 2\n",
      "documentation 3\n",
      "available 1\n",
      "from 1\n",
      "project 1\n",
      "site, 1\n",
      "at 2\n",
      "[\"Building 1\n",
      "Spark\"](https://spark.apache.org/docs/latest/building-spark.html). 1\n",
      "For 3\n",
      "development 1\n",
      "tips, 1\n",
      "info 1\n",
      "developing 1\n",
      "IDE, 1\n",
      "[\"Useful 1\n",
      "Developer 1\n",
      "Tools\"](https://spark.apache.org/developer-tools.html). 1\n",
      "Interactive 2\n",
      "Scala 2\n",
      "Shell 2\n",
      "The 1\n",
      "easiest 1\n",
      "way 1\n",
      "start 1\n",
      "through 1\n",
      "shell: 2\n",
      "./bin/spark-shell 1\n",
      "Try 1\n",
      "following 2\n",
      "command, 2\n",
      "which 2\n",
      "should 2\n",
      "return 2\n",
      "1,000,000,000: 2\n",
      "scala> 1\n",
      "spark.range(1000 2\n",
      "* 4\n",
      "1000 2\n",
      "1000).count() 2\n",
      "Python 2\n",
      "Alternatively, 1\n",
      "prefer 1\n",
      "use 3\n",
      "./bin/pyspark 1\n",
      "And 1\n",
      "run 7\n",
      ">>> 1\n",
      "Example 1\n",
      "Programs 1\n",
      "comes 1\n",
      "several 1\n",
      "sample 1\n",
      "programs 2\n",
      "`examples` 2\n",
      "directory. 1\n",
      "them, 1\n",
      "`./bin/run-example 1\n",
      "<class> 1\n",
      "[params]`. 1\n",
      "example: 1\n",
      "./bin/run-example 2\n",
      "SparkPi 2\n",
      "will 1\n",
      "Pi 1\n",
      "locally. 1\n",
      "MASTER 1\n",
      "environment 1\n",
      "variable 1\n",
      "when 1\n",
      "running 1\n",
      "examples 2\n",
      "submit 1\n",
      "cluster. 1\n",
      "be 2\n",
      "mesos:// 1\n",
      "or 3\n",
      "spark:// 1\n",
      "URL, 1\n",
      "\"yarn\" 1\n",
      "YARN, 1\n",
      "\"local\" 1\n",
      "locally 2\n",
      "thread, 1\n",
      "\"local[N]\" 1\n",
      "N 1\n",
      "threads. 1\n",
      "abbreviated 1\n",
      "class 2\n",
      "name 1\n",
      "package. 1\n",
      "instance: 1\n",
      "MASTER=spark://host:7077 1\n",
      "Many 1\n",
      "print 1\n",
      "usage 1\n",
      "help 1\n",
      "no 1\n",
      "params 1\n",
      "are 1\n",
      "given. 1\n",
      "Running 1\n",
      "Tests 1\n",
      "Testing 1\n",
      "first 1\n",
      "requires 1\n",
      "[building 1\n",
      "Spark](#building-spark). 1\n",
      "Once 1\n",
      "built, 1\n",
      "tests 2\n",
      "using: 1\n",
      "./dev/run-tests 1\n",
      "Please 4\n",
      "guidance 2\n",
      "how 3\n",
      "[run 1\n",
      "module, 1\n",
      "individual 1\n",
      "tests](https://spark.apache.org/developer-tools.html#individual-tests). 1\n",
      "There 1\n",
      "Kubernetes 1\n",
      "integration 1\n",
      "test, 1\n",
      "resource-managers/kubernetes/integration-tests/README.md 1\n",
      "A 1\n",
      "Note 1\n",
      "About 1\n",
      "Hadoop 3\n",
      "Versions 1\n",
      "uses 1\n",
      "core 1\n",
      "library 1\n",
      "talk 1\n",
      "HDFS 1\n",
      "other 1\n",
      "Hadoop-supported 1\n",
      "storage 1\n",
      "systems. 1\n",
      "Because 1\n",
      "protocols 1\n",
      "have 1\n",
      "changed 1\n",
      "different 1\n",
      "versions 1\n",
      "Hadoop, 2\n",
      "must 1\n",
      "against 1\n",
      "same 1\n",
      "version 1\n",
      "your 1\n",
      "cluster 1\n",
      "runs. 1\n",
      "refer 2\n",
      "[\"Specifying 1\n",
      "Version 1\n",
      "Enabling 1\n",
      "YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn) 1\n",
      "building 2\n",
      "particular 2\n",
      "distribution 1\n",
      "Hive 2\n",
      "Thriftserver 1\n",
      "distributions. 1\n",
      "Configuration 1\n",
      "[Configuration 1\n",
      "Guide](https://spark.apache.org/docs/latest/configuration.html) 1\n",
      "online 1\n",
      "overview 1\n",
      "configure 1\n",
      "Spark. 1\n",
      "Contributing 1\n",
      "review 1\n",
      "[Contribution 1\n",
      "guide](https://spark.apache.org/contributing.html) 1\n",
      "information 1\n",
      "get 1\n",
      "started 1\n",
      "contributing 1\n",
      "project. 1\n"
     ]
    }
   ],
   "source": [
    "#Create the wordcount\n",
    "for word, count in wordCounts.items():\n",
    "    cleanWord = word.encode('ascii', 'ignore')    #This take care of the encoding issues \n",
    "    if (cleanWord):\n",
    "        print(cleanWord.decode() + \" \" + str(count))\n",
    "                                                 #This method is not the best because we have a bunch of mixed up thing in there like \"Company)\" or \"entirely.\"\n",
    "                                                 #Because they can differenciate the punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
